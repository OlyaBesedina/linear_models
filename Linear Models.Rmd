---
title: "Linear Models"
author: "Olya Besedina"
data: "11/07/2019"
output: github_document
---
  
```{r setup, include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(viridis)

knitr::opts_chunk$set(
 	echo = TRUE,
 	warning = FALSE,
 	fig.width = 8, 
   fig.height = 6,
   out.width = "90%"
 )

options(
   ggplot2.continuous.colour = "viridis",
   ggplot2.continuous.fill = "viridis"
 )

 scale_colour_discrete = scale_colour_viridis_d
 scale_fill_discrete = scale_fill_viridis_d

 theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


# linear models 

```{r}

set.seed(1)

data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    boro = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(boro != "Staten Island") %>% 
  select(price, stars, boro, neighborhood, room_type)
```

An good place to start is to consider price as an outcome that may depend on rating and borough. We fit that initial model in the following code.

```{r}
fit = lm(price ~ stars + boro, data = nyc_airbnb)
```


I cat predictors (boro), r assumes that it is a factor abd will orginize them is some order. If order is not specified than R wil use alphabetical order - Bronx

```{r,eval= FALSE}
# not very descriptive
fit

# better summary
summary(fit)

# extract the coef
coef(fit)
summary(fit)$coef

# broom::tidy gives you the same thing but orginized in nice tibble

# broom::glance gives you other important info like AIC and BIC
fit %>% 
  broom::glance()

fit %>% 
  broom::tidy()
```

Tidy the results

```{r}
fit %>% 
  broom::tidy() %>% 
  #replave default boro with nice looking one
  mutate(
    term = str_replace(term, "boro", "Boro: ")
  ) %>%
  knitr::kable(digits = 3)
```

#Take a look at factors

fct_infreq - put cat var in order of how often they occur in data set
the most freq boro is manhattan 

```{r}
nyc_airbnb = 
  nyc_airbnb %>% 
  mutate(
    boro = fct_infreq(boro),
    room_type = fct_infreq(room_type)
  )
```

Refit the last model 

```{r}
# now ref is manhattan
fit = lm(price ~ stars + boro, data = nyc_airbnb)

fit %>% 
  broom::tidy()

```

## diagnostics - look at the residuals 

modelr is tidyverse adj
cut off at 500 to exclude outliers 

```{r}
modelr::add_residuals(nyc_airbnb, fit) %>% 
  ggplot(aes(x = stars, y = resid))+
  geom_point() +
  ylim(-500, 500)
```

Add fitted values, synonymous to predicted values

```{r}
modelr::add_predictions(nyc_airbnb, fit)
```


```{r}
fit_null = lm(price ~ stars + boro, data = nyc_airbnb)
fit_alt = lm(price ~ stars + boro + room_type, data = nyc_airbnb)
```

## Nesting

It is possible that going from 4 to 5 stars has different effect in Manh and Bronx
Check interactions
```{r}
fit_interaction = lm(price ~ stars * boro, data = nyc_airbnb)

fit_interaction %>% 
  broom::tidy()

# in man is stars are increased by 1, price will go up by ave $43
# in man starts effect the price a lot, in other boros not so much
```

Check interactions between stars and boro AND room type and boro

```{r}
nyc_airbnb %>% 
  lm(price ~ stars * boro + room_type * boro, data = .) %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)

```

Different model for each boro?

```{r}
nyc_airbnb %>% 
  filter(boro=="Brooklyn") %>% 
  lm(price ~ stars + room_type, data = .) %>% 
  broom::tidy()
```

Try to map this insted 

```{r}
nyc_airbnb %>% 
  # nest everything but boro
  nest(data = -boro) %>% 
  # fit lm for each boro as a new var
  mutate(
    models = map (.x = data, ~ lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(boro, results) %>% 
  unnest()

```


Let's nest neighborhoods

```{r}
manhattan_nest_lm_results = 
  nyc_airbnb %>% 
  filter( boro =="Manhattan") %>% 
  nest(data = -neighborhood) %>% 
  mutate(
    models = map (.x = data, ~ lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)
  ) %>% 
  select(neighborhood, results) %>% 
  unnest()
```


```{r}
manhattan_nest_lm_results %>% 
  filter(str_detect(term, "room_type")) %>% 
  ggplot(aes(x = neighborhood, y = estimate)) + 
  geom_point() + 
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```

check that one point in noho - does not really work

```{r}
nyc_airbnb %>% 
  filter(neighborhood == "NoHo", room_type == "Shared room")
```

Binary outcomes

```{r, eval = FALSE}
baltimore_df = 
  read_csv("data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. This uses the glm function with the family specified to account for the non-Gaussian outcome distribution.

```{r, eval = FALSE}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 
```

Many of the same tools we used to work with lm fits can be used for glm fits. The table below summaries the coefficients from the model fit; because logistic model estimates are log odds ratios, we include a step to compute odds ratios as well.

```{r, eval = FALSE}
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```
















